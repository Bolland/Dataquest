{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro \n",
    " In this mission, we'll explore how queries are executed in SQLite. After exploring this at a high level, we explore how to create and use indexes for better performance. As our data gets larger and our queries more complex, it's important to be able to tweak the queries we write and optimize a database's schema to ensure that we're getting results back quickly.\n",
    " \n",
    "## Data\n",
    "\n",
    "To explore database performance, we'll work with factbook.db, a SQLite database that contains information about each country in the world. We'll be working with the facts table in the database. Each row in facts represents a single country, and contains several columns, including:\n",
    "\n",
    "* name -- the name of the country.\n",
    "* area -- the total land and sea area of the country.\n",
    "* population -- the population of the country.\n",
    "* birth_rate -- the birth rate of the country.\n",
    "* created_at -- the date the record was created.\n",
    "\n",
    "\n",
    "### Explore dataset with PRAGMA\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'id', 'INTEGER', 1, None, 1)\n",
      "(1, 'code', 'varchar(255)', 1, None, 0)\n",
      "(2, 'name', 'varchar(255)', 1, None, 0)\n",
      "(3, 'area', 'integer', 0, None, 0)\n",
      "(4, 'area_land', 'integer', 0, None, 0)\n",
      "(5, 'area_water', 'integer', 0, None, 0)\n",
      "(6, 'population', 'integer', 0, None, 0)\n",
      "(7, 'population_growth', 'float', 0, None, 0)\n",
      "(8, 'birth_rate', 'float', 0, None, 0)\n",
      "(9, 'death_rate', 'float', 0, None, 0)\n",
      "(10, 'migration_rate', 'float', 0, None, 0)\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(\"factbook.db\")\n",
    "schema = conn.execute(\"PRAGMA table_info(facts);\").fetchall()\n",
    "\n",
    "for s in schema:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query plan\n",
    "\n",
    "We can use the EXPLAIN QUERY PLAN statement before any query we're running to get a high level query plan that would be performed. If you write a SELECT statement and place the EXPLAIN QUERY PLAN statement before it:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0, 'SCAN TABLE facts')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn.execute(\" EXPLAIN QUERY PLAN SELECT * FROM facts;\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SCAN TABLE` means that every row in entire table (facts) had to be accessed to evaluate the query. Since the SELECT query we wrote returns all of the columns and rows in the facts table, the entire table had to be accessed to get the results we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0, 'SCAN TABLE facts')]\n",
      "[(0, 0, 0, 'SCAN TABLE facts')]\n",
      "[(0, 0, 0, 'SCAN TABLE facts')]\n"
     ]
    }
   ],
   "source": [
    "query_plan_one = conn.execute(\"EXPLAIN QUERY PLAN SELECT * FROM facts WHERE area > 40000\").fetchall()\n",
    "print(query_plan_one)\n",
    "\n",
    "query_plan_two = conn.execute(\"EXPLAIN QUERY PLAN SELECT area FROM facts WHERE area > 40000\").fetchall()\n",
    "print(query_plan_two)\n",
    "\n",
    "query_plan_three = conn.execute(\"EXPLAIN QUERY PLAN SELECT * FROM facts WHERE name = 'Czech Republic'\").fetchall()\n",
    "print(query_plan_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying non-indexed columns\n",
    "You'll notice that all 3 query plans are exactly the same. The entire facts table had to be accessed to return the data we needed for all 3 queries. Even though all the queries asked for a subset of the facts table, SQLite still ends up scanning the entire table. Why is this? This is because of the way SQLite represents data.\n",
    "\n",
    "For the facts table, we set the id column as the primary key and SQLite uses this column to order the records in the database file. Since the rows are ordered by id, SQLite can search for a specific row based on it's id value using binary search. Unless we provide specific id values in the WHERE statement in the query, SQLite can't take advantage of binary search and has to instead scan the entire table, row by row. To return the results for the first 2 queries, SQLite has to:\n",
    "\n",
    "* access the first row in the table (lowest id value),\n",
    "* check if that row's value for area exceeds 40000 and store the row separately in a temporary collection if it is,\n",
    "move onto the next row,\n",
    "* check if that row's value for area exceeds 40000 and store the row separately in a temporary collection if it is,\n",
    "repeat moving and checking each row for the rest of the table,\n",
    "* return the final collection of rows that meet the criteria.\n",
    "\n",
    "--> Linear complexity **O(N)**\n",
    "\n",
    "\n",
    "\n",
    "### Querying indexed columns\n",
    "If we were instead interested in a row with a specific id value, like in the following query:\n",
    "\n",
    "```SQL\n",
    "SELECT * FROM facts WHERE id=15;\n",
    "```\n",
    "\n",
    "SQLite can use binary search to quickly find the corresponding row at that id value. Instead of performing a full table scan, SQLite would:\n",
    "\n",
    "* use binary search to find the first row where the id value matches 15 in **O(log N)** time complexity and store this row in a temporary collection,\n",
    "* advance to the next row to look for any more rows with the same id values and add those rows to the temporary collection, (If we set the id column to be a UNIQUE PRIMARY KEY when we created the schema, SQLite would stop searching when it found the instances that matched the id value. )\n",
    "* return the final collection of rows that matched.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 0, 'SEARCH TABLE facts USING INTEGER PRIMARY KEY (rowid=?)')]\n"
     ]
    }
   ],
   "source": [
    "query_plan_four = conn.execute(\"EXPLAIN QUERY PLAN SELECT * FROM facts WHERE id = 20;\").fetchall()\n",
    "print(query_plan_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQLite uses rowid to refer to the primary key of a table. The alias rowid will be displayed in the query plan, no matter what you name the primary key column for that table. Either `SCAN` or `SEARCH` will always appear at the start of the query explanation for SELECT queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Excurse: algorythmic complexity \n",
    "If we're working with a database containing millions of rows, binary search would be over a million times faster! While you may not notice major performance differences when working with a small, on-disk database, they become profound as you scale up the amount of data you work with. Many organizations work with databases contains billions or trillions of rows and understanding the time complexity of queries is important to avoid writing queries that take a long time to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "SQLite can take advantage of **speedy lookups when searching for a specific primary key**. Unfortunately, we don't always have the primary keys for the rows we're interested in beforehand. \n",
    "We could create a **separate table that's optimized for lookups by a different column** from the facts table instead of by the id. We can make the column we want to query by the primary key, so we get the speed benefits, and embed the id value from the facts table corresponding to that row. We call this table an index and each row in the index contains:\n",
    "\n",
    "* the **value** we want to be able to search by, **as the primary key**,\n",
    "* an **id value** for the corresponding row in facts.\n",
    "\n",
    "### Example\n",
    "When querying \n",
    "```SQL\n",
    "SELECT population FROM facts WHERE name = 'India';\n",
    "```\n",
    "\n",
    "SQLite would need to **perform a full table scan on facts** to find the specific row where the value for name was India. We can instead **create an index** that's ordered by name values (primary key) and where each row contains the corresponding row's id from the facts table. \n",
    "\n",
    "We can write a query that uses the **primary key, the country name, of the index table**, which we'll call `name_idx`, to look up the row we're interested in and then extract the id value for that row in facts. Then, we can write a separate query that uses the `id` value returned from the previous query to look up the specific row in the facts table that contains information on India and then return just the `population` value. (**Complexity O(N)**)\n",
    "\n",
    "Instead of performing a **single full table scan of facts**, SQLite would perform **two binary searches** on the index and then on the facts table. Both queries are taking advantage of the primary key for the index and the facts table to quickly return the results we want. (**Complexity 2xO(log(N))**)\n",
    "\n",
    "### Creating an index\n",
    "\n",
    "Instead of creating a separate table and updating it ourselves, we can **specify a column we want an index table for** and SQLite will take care of the rest. SQLite, and most databases, make it easy for you to create indexes for tables on columns we plan to query often. To create an index we use the `CREATE INDEX` statement. Here's the pseudo-code for that statement:\n",
    "\n",
    "```SQL\n",
    "CREATE INDEX IF NOT EXISTS index_name ON table_name(column_name);\n",
    "```\n",
    "\n",
    "(`IF NOT EXISTS` makes sure to not overwrite an existing index)\n",
    "\n",
    "An empty array will be returned when you run the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(conn.execute(\"CREATE INDEX IF NOT EXISTS area_idx ON facts(area);\").fetchall())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main benefit of having SQLite handle the maintenance of indexes we create is that the **indexes are used automatically when we execute a query whenever there will be any speed advantages**. As our queries become more complex, **letting SQLite decide how and when to use the indexes** we create helps us be much more productive.\n",
    "\n",
    " A table can have **many indexes**, and most tables in production environments usually do have many indexes. Every time you add or delete a row to the table, all of the **indexes will be updated**. If you edit the values in a row, SQLite will figure out which indexes are affected by the changes and update those indexes.\n",
    " \n",
    " ### Tradeoffs\n",
    " \n",
    " While creating indexes gives us tremendous speed benefits, **they come at the cost of space**. Each index needs to be stored in the database file. \n",
    " In addition, **adding, editing, and deleting rows takes longer** since each of the affected indexes need to be updated. Since indexes can be created after a table is created, it's recommended to only create an index when you find yourself querying on a specific column frequently. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
